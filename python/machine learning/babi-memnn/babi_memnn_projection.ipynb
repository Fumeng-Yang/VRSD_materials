{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"babi_memnn_projection_umap.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"KZz_KsxQ439D","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import print_function"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oCpCpbLPOqnT","colab_type":"code","colab":{}},"cell_type":"code","source":["import umap\n","\n","umap3d = umap.UMAP(n_components=3, n_neighbors = 15, min_dist=0.1, metric = 'minkowski', random_state = 31)\n","umap2d = umap.UMAP(n_components=2, n_neighbors = 15, min_dist=0.1, metric = 'minkowski', random_state = 31)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jx4rcxUi5VbI","colab_type":"code","outputId":"7c406904-df91-43eb-bfd5-913dd2d3773d","executionInfo":{"status":"ok","timestamp":1547043681121,"user_tz":300,"elapsed":48532,"user":{"displayName":"Fumeng Yang","photoUrl":"https://lh6.googleusercontent.com/-aVAaDqsi-BQ/AAAAAAAAAAI/AAAAAAAAIqA/KOrwB0RzHnA/s64/photo.jpg","userId":"01966251488146498713"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["import getpass\n","user = getpass.getuser()\n","\n","if user == 'root':\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"0nnL1RPu5Wt6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a91b10b4-9a5c-43d2-8c6e-12c79c016f6b","executionInfo":{"status":"ok","timestamp":1547043682405,"user_tz":300,"elapsed":49784,"user":{"displayName":"Fumeng Yang","photoUrl":"https://lh6.googleusercontent.com/-aVAaDqsi-BQ/AAAAAAAAAAI/AAAAAAAAIqA/KOrwB0RzHnA/s64/photo.jpg","userId":"01966251488146498713"}}},"cell_type":"code","source":["################################################################\n","# import blocks\n","################################################################\n","# I/O\n","import os\n","\n","# agrs\n","import sys\n","\n","# load json\n","import json\n","\n","# garabage collection\n","import gc\n","\n","import numpy as np\n","\n","import keras\n","\n","from keras.models import Sequential, Model\n","from keras.layers.embeddings import Embedding\n","from keras.layers import Input, Activation, Dense, Permute, Dropout\n","from keras.layers import add, dot, concatenate\n","from keras.layers import LSTM\n","from keras.utils.data_utils import get_file\n","from keras.preprocessing.sequence import pad_sequences\n","from functools import reduce\n","import tarfile\n","import numpy as np\n","import re\n","\n","# split folds\n","from sklearn.model_selection import KFold\n","\n","# tsne setups\n","from sklearn.manifold import TSNE\n","\n","t_steps = 600\n","t_perplexity = 30\n","t_learning_rate = 150\n","tsne2 = TSNE(n_components=2,\n","             n_iter=t_steps,\n","             n_iter_without_progress=t_steps,\n","             perplexity=t_perplexity,\n","             learning_rate = t_learning_rate,\n","             random_state = 31)\n","tsne3 = TSNE(n_components=3,\n","             n_iter=t_steps,\n","             perplexity=t_perplexity,\n","             n_iter_without_progress=t_steps,\n","             learning_rate = t_learning_rate,\n","             random_state = 31)\n","\n","# plotting parameters\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import statistics, math, numbers, numpy\n","import matplotlib.colors as colors\n","from operator import itemgetter\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"eScHbqGWz8-c","colab_type":"code","colab":{}},"cell_type":"code","source":["import umap\n","\n","umap3 = umap.UMAP(n_components=3, n_neighbors = 15, min_dist=0.1, metric = 'minkowski', random_state = 31)\n","umap2 = umap.UMAP(n_components=2, n_neighbors = 15, min_dist=0.1, metric = 'minkowski', random_state = 31)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AUkDcbJ35fFL","colab_type":"code","colab":{}},"cell_type":"code","source":["################################################################\n","# parameters setting\n","################################################################\n","# fix random seeds\n","from numpy.random import seed\n","seed(3)\n","\n","from tensorflow import set_random_seed\n","set_random_seed(31)\n","\n","\n","BATCH_SIZE=32\n","EPOCHS=200\n","VALIDATION_SPLIT=0.2\n","TARGET_LAYER = -2\n","VIS_NUM_POINTS = 1000\n","\n","challenges = {\n","    # QA1 with 10,000 samples, single_supporting_fact_10k\n","    'babi-q1': 'tasks_1-20_v1-2/en-10k/qa1_' \n","                                  'single-supporting-fact_{}.txt',\n","    # QA2 with 10,000 samples, two_supporting_facts_10k\n","    'babi-q2': 'tasks_1-20_v1-2/en-10k/qa2_'\n","                                'two-supporting-facts_{}.txt',\n","}\n","\n","DATA_SET = challenge_type = sys.argv[1] if len(sys.argv) > 1 and sys.argv[1] != '-f' else 'babi-q1'\n","challenge = challenges[challenge_type]\n","\n","# check points\n","path_header = '/content/gdrive/My Drive/Colab Notebooks/babi-memnn/' if user == 'root' else ''\n","checkpoint_path = path_header + 'saved_models/' + DATA_SET + '-e{epoch:04d}.ckpt'\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# data path\n","data_dir = path_header + 'saved_projections'\n","\n","# colors\n","tableau10 = [\"#4E79A7\", \"#F28E2B\", \"#E15759\", \"#76B7B2\", \"#59A14F\",\n","            \"#EDC948\", \"#B07AA1\", \"#FF9DA7\", \"#9C755F\", \"#BAB0AC\"]\n","\n","tableau20 = [\"#4E79A7\", \"#A0CBE8\", \"#F28E2B\", \"#FFBE7D\", \"#59A14F\",\n","             \"#8CF17D\", \"#B6992D\", \"#EDC948\",  \"#499894\", \"#76B7B2\",\n","             \"#E15759\", \"#FF9D9A\", \"#79706E\", \"#BAB0AC\", \"#D37295\",\n","             \"#FABFD2\", \"#B07AA1\", \"#D4A6C8\", \"#9D7660\", \"#D7B5A6\"]\n","\n","tableau22 = [\"#4E79A7\", \"#A0CBE8\", \"#F28E2B\", \"#FFBE7D\", \"#59A14F\",\n","             \"#8CF17D\", \"#B6992D\", \"#EDC948\",  \"#499894\", \"#76B7B2\",\n","             \"#E15759\", \"#FF9D9A\", \"#79706E\", \"#BAB0AC\", \"#D37295\",\n","             \"#FABFD2\", \"#B07AA1\", \"#D4A6C8\", \"#9D7660\", \"#D7B5A6\", \n","             \"#FF9DA7\", \"#9C755F\"]\n","\n","tableau10_colors = colors.ListedColormap(tableau10)\n","tableau20_colors = colors.ListedColormap(tableau20)\n","tableau22_colors = colors.ListedColormap(tableau22)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dwZiWT-y5gZk","colab_type":"code","colab":{}},"cell_type":"code","source":["def tokenize(sent):\n","    '''Return the tokens of a sentence including punctuation.\n","    >>> tokenize('Bob dropped the apple. Where is the apple?')\n","    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n","    '''\n","    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n","\n","\n","def parse_stories(lines, only_supporting=False):\n","    '''Parse stories provided in the bAbi tasks format\n","    If only_supporting is true, only the sentences\n","    that support the answer are kept.\n","    '''\n","    data = []\n","    story = []\n","    for line in lines:\n","        line = line.decode('utf-8').strip()\n","        nid, line = line.split(' ', 1)\n","        nid = int(nid)\n","        if nid == 1:\n","            story = []\n","        if '\\t' in line:\n","            q, a, supporting = line.split('\\t')\n","            q = tokenize(q)\n","            if only_supporting:\n","                # Only select the related substory\n","                supporting = map(int, supporting.split())\n","                substory = [story[i - 1] for i in supporting]\n","            else:\n","                # Provide all the substories\n","                substory = [x for x in story if x]\n","            data.append((substory, q, a))\n","            story.append('')\n","        else:\n","            sent = tokenize(line)\n","            story.append(sent)\n","    return data\n","\n","\n","def get_stories(f, only_supporting=False, max_length=None):\n","    '''Given a file name, read the file,\n","    retrieve the stories,\n","    and then convert the sentences into a single story.\n","    If max_length is supplied,\n","    any stories longer than max_length tokens will be discarded.\n","    '''\n","    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n","    flatten = lambda data: reduce(lambda x, y: x + y, data)\n","    data = [(flatten(story), q, answer) for story, q, answer in data\n","            if not max_length or len(flatten(story)) < max_length]\n","    return data\n","\n","\n","def vectorize_stories(data):\n","    inputs, queries, answers = [], [], []\n","    for story, query, answer in data:\n","        inputs.append([word_idx[w] for w in story])\n","        queries.append([word_idx[w] for w in query])\n","        answers.append(word_idx[answer])\n","    return (pad_sequences(inputs, maxlen=story_maxlen),\n","            pad_sequences(queries, maxlen=query_maxlen),\n","            np.array(answers))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TluR4sVN5iUF","colab_type":"code","outputId":"b62f7a55-9096-40b0-838e-911ff16b2436","executionInfo":{"status":"ok","timestamp":1547043683630,"user_tz":300,"elapsed":50935,"user":{"displayName":"Fumeng Yang","photoUrl":"https://lh6.googleusercontent.com/-aVAaDqsi-BQ/AAAAAAAAAAI/AAAAAAAAIqA/KOrwB0RzHnA/s64/photo.jpg","userId":"01966251488146498713"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["try:\n","    path = get_file('babi-tasks-v1-2.tar.gz',\n","                    origin='https://s3.amazonaws.com/text-datasets/'\n","                           'babi_tasks_1-20_v1-2.tar.gz')\n","except:\n","    print('Error downloading dataset, please download it manually:\\n'\n","          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n","          '.tar.gz\\n'\n","          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n","    raise"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n","11747328/11745123 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"IOm0NN075j7y","colab_type":"code","outputId":"3c259d78-5ce8-4b90-de2e-cc9ba983b8f1","executionInfo":{"status":"ok","timestamp":1547043685212,"user_tz":300,"elapsed":52501,"user":{"displayName":"Fumeng Yang","photoUrl":"https://lh6.googleusercontent.com/-aVAaDqsi-BQ/AAAAAAAAAAI/AAAAAAAAIqA/KOrwB0RzHnA/s64/photo.jpg","userId":"01966251488146498713"}},"colab":{"base_uri":"https://localhost:8080/","height":493}},"cell_type":"code","source":["################################################################\n","# processing data\n","################################################################\n","\n","print('Extracting stories for the challenge:', challenge_type)\n","with tarfile.open(path) as tar:\n","    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n","    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n","\n","vocab = set()\n","for story, q, answer in train_stories + test_stories:\n","    vocab |= set(story + q + [answer])\n","vocab = sorted(vocab)\n","\n","# Reserve 0 for masking via pad_sequences\n","vocab_size = len(vocab) + 1\n","story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n","query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n","\n","print('-')\n","print('Vocab size:', vocab_size, 'unique words')\n","print('Story max length:', story_maxlen, 'words')\n","print('Query max length:', query_maxlen, 'words')\n","print('Number of training stories:', len(train_stories))\n","print('Number of test stories:', len(test_stories))\n","print('-')\n","print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n","print(train_stories[0])\n","print('-')\n","print('Vectorizing the word sequences...')\n","\n","word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n","inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n","inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n","\n","print('-')\n","print('inputs: integer tensor of shape (samples, max_length)')\n","print('inputs_train shape:', inputs_train.shape)\n","print('inputs_test shape:', inputs_test.shape)\n","print('-')\n","print('queries: integer tensor of shape (samples, max_length)')\n","print('queries_train shape:', queries_train.shape)\n","print('queries_test shape:', queries_test.shape)\n","print('-')\n","print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n","print('answers_train shape:', answers_train.shape)\n","print('answers_test shape:', answers_test.shape)\n","print('-')\n","print('Compiling...')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Extracting stories for the challenge: babi-q1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n","  return _compile(pattern, flags).split(string, maxsplit)\n"],"name":"stderr"},{"output_type":"stream","text":["-\n","Vocab size: 22 unique words\n","Story max length: 68 words\n","Query max length: 4 words\n","Number of training stories: 10000\n","Number of test stories: 1000\n","-\n","Here's what a \"story\" tuple looks like (input, query, answer):\n","(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n","-\n","Vectorizing the word sequences...\n","-\n","inputs: integer tensor of shape (samples, max_length)\n","inputs_train shape: (10000, 68)\n","inputs_test shape: (1000, 68)\n","-\n","queries: integer tensor of shape (samples, max_length)\n","queries_train shape: (10000, 4)\n","queries_test shape: (1000, 4)\n","-\n","answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n","answers_train shape: (10000,)\n","answers_test shape: (1000,)\n","-\n","Compiling...\n"],"name":"stdout"}]},{"metadata":{"id":"vy8YdNgZ5l1P","colab_type":"code","colab":{}},"cell_type":"code","source":["# load training history\n","with open(checkpoint_dir + '/' + DATA_SET + '-history.json') as f:\n","    hist_json = json.load(f)\n","    history = hist_json['history']\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jZMsTvW_E4oz","colab_type":"code","outputId":"5d5ed114-7c8d-4146-ddb2-3e711bd31fd3","executionInfo":{"status":"ok","timestamp":1547043690205,"user_tz":300,"elapsed":57454,"user":{"displayName":"Fumeng Yang","photoUrl":"https://lh6.googleusercontent.com/-aVAaDqsi-BQ/AAAAAAAAAAI/AAAAAAAAIqA/KOrwB0RzHnA/s64/photo.jpg","userId":"01966251488146498713"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["NUM_CLASSES = len(set(answers_test))\n","print(NUM_CLASSES)\n","desc_num_points = str(int(VIS_NUM_POINTS / 1000)) + 'k'"],"execution_count":11,"outputs":[{"output_type":"stream","text":["6\n"],"name":"stdout"}]},{"metadata":{"id":"V8QBCaQLPLzi","colab_type":"code","colab":{}},"cell_type":"code","source":["p2 = umap2\n","p3 = umap3\n","p_name = 'umap'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vx54_WPECSjk","colab_type":"code","colab":{}},"cell_type":"code","source":["def write_file(content, path):\n","    file = open(path,'w+')\n","    file.write(content)\n","    file.close()\n","   \n","    \n","def write_fig(p2d, p3d, labels, fig_path):\n","    save_fig = plt.gcf()\n","    fig = plt.figure(figsize=(15, 7))\n","\n","    ax = fig.add_subplot(1, 2, 1, projection='3d')\n","\n","    ax.scatter(xs = p3d[:,0],\n","           ys = p3d[:,1],\n","           zs = p3d[:,2],\n","           c = labels,\n","           cmap = tableau10_colors, alpha = 1)\n","    plt.title(p_name + ' 3D after training')\n","\n","    fig.add_subplot(1, 2, 2)\n","\n","    plt.scatter(x = p2d[:,0],\n","            y = p2d[:,1],\n","            c = labels,\n","            cmap = tableau10_colors, alpha= 1)\n","    plt.title(p_name + ' 2D after training')\n","\n","    plt.draw()\n","    fig.savefig(fig_path, dpi=100)\n","    plt.close(fig)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C_tsKQ55OyqP","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"4yPPTeM96VDV","colab_type":"code","outputId":"944cbd13-be05-4c19-badd-ba713b7a14d6","executionInfo":{"status":"ok","timestamp":1547043941608,"user_tz":300,"elapsed":308794,"user":{"displayName":"Fumeng Yang","photoUrl":"https://lh6.googleusercontent.com/-aVAaDqsi-BQ/AAAAAAAAAAI/AAAAAAAAIqA/KOrwB0RzHnA/s64/photo.jpg","userId":"01966251488146498713"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"cell_type":"code","source":["################################################################\n","#compute projections\n","################################################################\n","\n","file_names = \"\"\n","test_acc_records = \"\"\n","training_acc_records = \"\"\n","\n","    \n","x_ = inputs_test\n","q_ = queries_test\n","y_ = answers_test\n","j = 0\n","y_labels = y_.copy()\n","\n","# for each model\n","for e in np.arange(184, EPOCHS):\n","    \n","    model_path = checkpoint_dir + '/' + DATA_SET + '-e%04d.ckpt' % (e + 1)\n","    file_name = DATA_SET + '-e%04d'% (e + 1) + '-f%d'%j + '-' + desc_num_points + '-l%d'%TARGET_LAYER + '-' + p_name + '.txt'\n","    fig_path = path_header + 'saved_figs/' + DATA_SET + '-e%04d'% (e + 1) + '-f%d'%j + '-' + desc_num_points + '-l%d'%TARGET_LAYER + '-' + p_name + '.png'\n","\n","    # load model\n","    print('.........loading ' + model_path + ' for ' + p_name)\n","    saved_model = keras.models.load_model(model_path)\n","    \n","    target_layer_model = Model(inputs=saved_model.input,\n","                               outputs=saved_model.layers[TARGET_LAYER].output)\n","\n","    \n","    y_target = target_layer_model.predict([x_, q_])\n","    score = saved_model.evaluate([x_, q_], y_, verbose = 0, batch_size=BATCH_SIZE)\n"," \n","    y_pred = saved_model.predict([x_, q_], batch_size=BATCH_SIZE)\n","    y_pred_labels = [int(np.argmax(y)) for y in y_pred]\n","    y_correctness = list(map(lambda x, y: True if x == y else False, y_pred_labels, y_labels))\n","    \n","    \n","    y_target_p2d = p2.fit_transform(y_target)\n","    y_target_p3d = p3.fit_transform(y_target)\n","    \n","    #print('t-SNE steps %d, %d' % (tsne2.n_iter_ , tsne3.n_iter_))\n","    #print('t-SNE errors %f, %f' % (tsne2.kl_divergence_ , tsne3.kl_divergence_))\n","\n","    # normalizing\n","    y_target_p2d_normed = y_target_p2d.transpose()\n","    y_target_p2d_normed = numpy.array([[(d - numpy.mean(d))/ numpy.std(d)] for d in y_target_p2d_normed]).reshape(2, VIS_NUM_POINTS).transpose()\n","\n","    y_target_p3d_normed = y_target_p3d.transpose()\n","    y_target_p3d_normed = numpy.array([[(d - numpy.mean(d))/ numpy.std(d)] for d in y_target_p3d_normed]).reshape(3, VIS_NUM_POINTS).transpose()\n","    \n","\n","    output_path = data_dir + '/' + file_name\n","    OUTPUT = \"\"\n","\n","    OUTPUT += 'data_set\\n' + DATA_SET + '\\n' + \\\n","    'num_classes\\n' + str(NUM_CLASSES) + '\\n' + \\\n","    'data_points\\n' + str(VIS_NUM_POINTS) + '\\n' +\\\n","     \\\n","    'model\\n' + model_path + '\\n' +\\\n","     \\\n","    'color_schema\\n' + ','.join(tableau10) + '\\n' +\\\n","    \\\n","    'test_accuracy\\n' + str(score[1]) + '\\n' +\\\n","    'test_loss\\n' + str(score[0]) + '\\n' +\\\n","    \\\n","    'train_accuracy\\n' + str(history['acc'][e]) + '\\n' +\\\n","    'train_loss\\n' + str(history['loss'][e]) + '\\n' +\\\n","    \\\n","    'val_accuracy\\n' + str(history['val_acc'][e]) + '\\n' +\\\n","    'val_loss\\n' + str(history['val_loss'][e]) + '\\n' +\\\n","    \\\n","    'labels\\n' + ','.join([str(d) for d in y_labels]) + '\\n' +\\\n","    'pred_labels\\n' + ','.join([str(d) for d in y_pred_labels]) + '\\n' +\\\n","    'pred_correctness\\n' + ','.join([str(d) for d in y_correctness]) + '\\n' +\\\n","    \\\n","    'p2d\\n' + ';'.join([str(d[0]) + ',' + str(d[1]) for d in y_target_p2d_normed])  + '\\n' +\\\n","    \\\n","    'p3d\\n' + ';'.join([str(d[0]) + ',' + str(d[1])  + ',' + str(d[2]) for d in y_target_p3d_normed])  + '\\n'\n","\n","    file_names += file_name + ','\n","    test_acc_records += str(score[1]) + ','\n","    training_acc_records +=  str(history['acc'][e]) + ','\n","        \n","    file = open(output_path,'w+')\n","    file.write(OUTPUT)\n","    file.close()\n","\n","    write_file(OUTPUT, output_path)\n","    write_fig(y_target_p2d_normed, y_target_p3d_normed, y_labels, fig_path)\n","        \n","    del y_target, y_pred, y_pred_labels, y_correctness, y_target_p2d, y_target_p3d, y_target_p2d_normed, y_target_p3d_normed\n","    gc.collect()\n","    \n","    \n","# write accuracy files\n","accuracy_path = data_dir + '/'+ 'accuracy.txt'\n","accuracy_content = file_names[:-1] + '\\n' + test_acc_records[:-1] +'\\n' + training_acc_records[:-1]\n","write_file(accuracy_content, accuracy_path)"],"execution_count":14,"outputs":[{"output_type":"stream","text":[".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0185.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0186.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0187.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0188.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0189.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0190.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0191.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0192.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0193.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0194.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0195.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0196.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0197.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0198.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0199.ckpt for umap\n",".........loading /content/gdrive/My Drive/Colab Notebooks/babi-memnn/saved_models/babi-q1-e0200.ckpt for umap\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<matplotlib.figure.Figure at 0x7fb28df3fd30>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"lwSx09C2Etpp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}